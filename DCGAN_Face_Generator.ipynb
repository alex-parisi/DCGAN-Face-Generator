{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-Face-Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9C6bTilHY6271N0BfmXW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-parisi/DCGAN-Face-Generator/blob/main/DCGAN_Face_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Faces with a Deep Convolutional Generative Adversarial Network (DCGAN)\n",
        "\n",
        "A DCGAN uses two networks (discriminator and generator) working against one another in attempt to generate images that could pass as \"authentic\". A discriminator network is trained to determine whether or not an inputted image is a genuine image or an image generated by the generator network - which is attempting to generate images that will deceive the discriminator.\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "_2N3EQgLF2kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discriminator has a relatively standard layout in image recognition, and consists of an input layer, three convolution layers, a dropout layer, and then a fully connected layer. The convolution layers use a leakly ReLu activation function, and the fully connected layer uses a sigmoid activation function.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1JOLpN18ANTYuiz6c5An_T5body47yFZw'>\n",
        "\n",
        "The above image shows the layout of the generator in this DCGAN. A vector of random noise is upscaled through convolution layers until the appropriate image size is reached."
      ],
      "metadata": {
        "id": "6bKCgZKyQE-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3OUh-MAJL4Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "import random\n",
        "import glob\n",
        "import imageio\n",
        "import cv2\n",
        "from google.colab import auth"
      ],
      "metadata": {
        "id": "1z8hKLvS2iZG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Authenticate and Initiate TPU's\n",
        "In order to connect to Google Cloud Services (GCS) to load the dataset, you must authenticate your Google account. Run the snippet below and follow the link, then paste the access key into the input box and press Enter."
      ],
      "metadata": {
        "id": "oYgWY6rtLmex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "5hALXmJK2Z70"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running this, ensure the Google Colab notebook is set to use TPU's. Go to \"Edit\", then \"Notebook settings\", and set the Hardware Accelerator to \"TPU\".\n",
        "\n",
        "<br>This will initiate the TPU's, which will offer this program almost an 800% increase in speed compared to running locally on a GTX 970."
      ],
      "metadata": {
        "id": "OOZzX3Z2PV62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "4nOmt7sL2sgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset\n",
        "\n",
        "In order to use TPU's on Google Colab, you cannot use a local filesystem for data - you **must** use a GCS bucket to hold your images. Note that also when using Keras, the dataset object cannot be used to load images directly from a GCS bucket. Therefore, we must first convert the dataset to a .tfrecords format file and upload that to the GCS bucket, which Keras can then convert to a dataset object and use for training. This only needs to be performed once, as once the .tfrecords file is uploaded to the GCS bucket, we can refer to it as long as the training set doesn't change.\n",
        "\n",
        "First, you must sign up for GCS [here](https://cloud.google.com/). There are free options available - personally I am using the 90 day free trial.\n",
        "\n",
        "Then, access the \"Storage\" section [here](https://cloud.google.com/storage) and create a bucket. This is where you will upload your .tfrecords file.\n",
        "\n",
        "Remember the name that you use for the bucket, as the link you will use throughout is \"gs://< bucket_name>\""
      ],
      "metadata": {
        "id": "wRBUIBEYL7BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download celeb-a dataset"
      ],
      "metadata": {
        "id": "xo4oVFWFQBtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"celeba_gan\")\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)"
      ],
      "metadata": {
        "id": "B31-Qwsbowhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract to local filesystem in runtime environment"
      ],
      "metadata": {
        "id": "_qMBLP65QMJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ],
      "metadata": {
        "id": "dWwaLT2UQKz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assemble list of filenames in celeb-a dataset"
      ],
      "metadata": {
        "id": "8dHzEthcQR05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_pics = []\n",
        "for path, subdirs, files in os.walk(os.path.join(os.getcwd(), 'celeba_gan')):\n",
        "  for name in files:\n",
        "    if name.startswith('.'):\n",
        "      continue\n",
        "    if '.png' in name or '.jpg' in name:\n",
        "      in_pics.append(os.path.join(path, name))"
      ],
      "metadata": {
        "id": "_l4_fyDGdTzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle filenames"
      ],
      "metadata": {
        "id": "tZmw4mx6QTZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(in_pics)\n",
        "print('enumerated pics: ', len(in_pics))"
      ],
      "metadata": {
        "id": "JzrzayDjd1iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write image data in each filename to a .tfrecords file"
      ],
      "metadata": {
        "id": "ONDmggaGQVkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TFRecord_write_file = os.path.join(os.getcwd(), 'celeba_gan.tfrecords')\n",
        "print('Writing TFRecord', TFRecord_write_file)\n",
        "with tf.io.TFRecordWriter(TFRecord_write_file) as writer:\n",
        "    for i in range(len(in_pics)):\n",
        "        with tf.io.gfile.GFile(in_pics[i], 'rb') as fid:\n",
        "            img = fid.read()\n",
        "        example = tf.train.Example(\n",
        "                        features=tf.train.Features(\n",
        "                            feature={\n",
        "                                'image': tf.train.Feature(bytes_list = tf.train.BytesList(value=[img])),\n",
        "                            }))\n",
        "        writer.write(example.SerializeToString())\n",
        "print('Writing TFRecord done')"
      ],
      "metadata": {
        "id": "qEvtPgLHh63q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload .tfrecords file to the GCS bucket you created.\n",
        "<br>Replace: ```gs://celeba-alexp/celeba_gan.tfrecords```\n",
        "<br>With: ```gs://< bucket_name>/celeba_gan.tfrecords```\n"
      ],
      "metadata": {
        "id": "qj6KNdIqQXRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp /content/celeba_gan.tfrecords gs://celeba-alexp/celeba_gan.tfrecords"
      ],
      "metadata": {
        "id": "1f2l-T47iQft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assemble dataset from GCS bucket"
      ],
      "metadata": {
        "id": "VffhZqbcPdlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the extract function to parse the .tfrecords file and load the dataset within the TPU scope"
      ],
      "metadata": {
        "id": "LpyTObSyRTWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace: ```gs://celeba-alexp/celeba_gan.tfrecords```\n",
        "<br> With: ```gs://< bucket_name>/celeba_gan.tfrecords```"
      ],
      "metadata": {
        "id": "lv-3J7jLW_FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TFRecord_extract_fn(data_record):\n",
        "    features = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    sample = tf.io.parse_single_example(data_record, features)\n",
        "    sample = tf.image.decode_jpeg(sample['image'], channels=3)\n",
        "    sample = tf.image.convert_image_dtype(sample, tf.float32)\n",
        "    sample = tf.image.resize(sample, [64, 64])\n",
        "    return sample\n",
        "\n",
        "with strategy.scope():\n",
        "  dataset = tf.data.TFRecordDataset('gs://celeba-alexp/celeba_gan.tfrecords')\n",
        "  dataset = dataset.map(TFRecord_extract_fn)\n",
        "  dataset = dataset.batch(32)"
      ],
      "metadata": {
        "id": "4HjcAquukjxj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Models\n",
        "\n",
        "As stated above, the DCGAN is split into two competing neural networks: a discriminator and a generator. The discriminator attempts to determine whether or not an inputted image is authentic, i.e. a member of the original dataset, or is a fake generated by the generator. The generator attempts to create an image authentic enough to trick the discriminator into making an incorrect classification."
      ],
      "metadata": {
        "id": "IflNfmuWRig4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The discriminator has the shape:\n",
        "\n",
        "*   (None, 64, 64, 3)\n",
        "*   (None, 32, 32, 64)\n",
        "*   (None, 16, 16, 128)\n",
        "*   (None, 8, 8, 128)\n",
        "*   (None, 8192)\n",
        "*   (None, 1)\n",
        "\n",
        "The generator has the shape:\n",
        "\n",
        "*   (None, 128)\n",
        "*   (None, 4, 4, 1024)\n",
        "*   (None, 8, 8, 512)\n",
        "*   (None, 16, 16, 256)\n",
        "*   (None, 32, 32, 128)\n",
        "*   (None, 64, 64, 3)\n",
        "\n",
        "<br>Note that \"None\" is the batch size, which in this case is 32\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "42BirHjMSHxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  discriminator = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=(64, 64, 3)),\n",
        "          layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "          layers.LeakyReLU(alpha=0.2),\n",
        "          layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "          layers.LeakyReLU(alpha=0.2),\n",
        "          layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "          layers.LeakyReLU(alpha=0.2),\n",
        "          layers.Flatten(),\n",
        "          layers.Dropout(0.2),\n",
        "          layers.Dense(1, activation=\"sigmoid\"),\n",
        "      ],\n",
        "      name=\"discriminator\",\n",
        "  )\n",
        "\n",
        "  latent_dim = 128\n",
        "\n",
        "  generator = keras.Sequential(\n",
        "      [\n",
        "          keras.Input(shape=(latent_dim,)),\n",
        "          layers.Dense(4 * 4 * 1024),\n",
        "          layers.Reshape((4, 4, 1024)),\n",
        "          layers.Conv2DTranspose(512, kernel_size=5, strides=2, padding=\"same\"),\n",
        "          layers.LeakyReLU(alpha=0.2),\n",
        "          layers.Conv2DTranspose(256, kernel_size=5, strides=2, padding=\"same\"),\n",
        "          layers.LeakyReLU(alpha=0.2),\n",
        "          layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding=\"same\"),\n",
        "          layers.LeakyReLU(alpha=0.2),\n",
        "          layers.Conv2DTranspose(3, kernel_size=5, strides=2, padding=\"same\", activation=\"sigmoid\"),\n",
        "      ],\n",
        "      name=\"generator\",\n",
        "  )"
      ],
      "metadata": {
        "id": "ic7L8aen2wDC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the GAN Network. This step is complicated, as I am using a custom training loop. This is a necessary step because we need to establish a shared loss function - this ensures that as the discriminator gets better at discriminating, the generator will concurrently get better at generating."
      ],
      "metadata": {
        "id": "dZCFoCmMVebq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IGi-EFt8rlPm"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():  \n",
        "  class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "      super(GAN, self).__init__()\n",
        "      self.discriminator = discriminator\n",
        "      self.generator = generator\n",
        "      self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "      super(GAN, self).compile()\n",
        "      self.d_optimizer = d_optimizer\n",
        "      self.g_optimizer = g_optimizer\n",
        "      self.loss_fn = loss_fn\n",
        "      self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "      self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "      return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "      # Sample random points in the latent space\n",
        "      batch_size = tf.shape(real_images)[0]\n",
        "      random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "      # Decode them to fake images\n",
        "      generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "      # Combine them with real images\n",
        "      combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "      # Assemble labels discriminating real from fake images\n",
        "      labels = tf.concat(\n",
        "        [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "      )\n",
        "      # Add random noise to the labels\n",
        "      labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "      # Train the discriminator\n",
        "      with tf.GradientTape() as tape:\n",
        "        predictions = self.discriminator(combined_images)\n",
        "        d_loss = self.loss_fn(labels, predictions)\n",
        "      grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "      self.d_optimizer.apply_gradients(\n",
        "        zip(grads, self.discriminator.trainable_weights)\n",
        "      )\n",
        "\n",
        "      # Sample random points in the latent space\n",
        "      random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "      # Assemble labels that say \"all real images\"\n",
        "      misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "      # Train the generator\n",
        "      with tf.GradientTape() as tape:\n",
        "        predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "        g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "      grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "      self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "      # Update metrics\n",
        "      self.d_loss_metric.update_state(d_loss)\n",
        "      self.g_loss_metric.update_state(g_loss)\n",
        "      return {\n",
        "        \"d_loss\": self.d_loss_metric.result(),\n",
        "        \"g_loss\": self.g_loss_metric.result(),\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a monitor function that executes at the end of each epoch. This will save a checkpoint of the model (if the epoch is a multiple of 5) and will generate 10 images from the generator and save them. The seed used to generate these images is saved alongside model information when a checkpoint occurs."
      ],
      "metadata": {
        "id": "V5qGpxPYWIU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():  \n",
        "  class GANMonitor(keras.callbacks.Callback):\n",
        "      def __init__(self, num_img=3, latent_dim=128):\n",
        "          self.num_img = num_img\n",
        "          self.latent_dim = latent_dim\n",
        "\n",
        "      def on_epoch_end(self, epoch, logs=None):\n",
        "          if (epoch + 1) % 5 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix, options=local_device_option)\n",
        "          generated_images = self.model.generator(random_latent_vectors_monitor)\n",
        "          generated_images *= 255\n",
        "          generated_images.numpy()\n",
        "          for i in range(self.num_img):\n",
        "              img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "              img.save(\"generated_img_%03d_%d.png\" % (epoch + 0, i))"
      ],
      "metadata": {
        "id": "SuurXzVY25k2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, create the GAN model"
      ],
      "metadata": {
        "id": "MHjIUNr6Wa2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)"
      ],
      "metadata": {
        "id": "VWGIFnRnj_q_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Fr-cxh6IWhC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup checkpoints that will save the model information and generator seed for later use, or in case training gets interrupted."
      ],
      "metadata": {
        "id": "dZPSvaxdXuNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_latent_vectors_monitor = tf.random.normal(shape=(10, latent_dim))\n",
        "gen_var = tf.Variable(random_latent_vectors_monitor)\n",
        "\n",
        "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "generator_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator,\n",
        "                                 gen_var=gen_var,\n",
        "                                 gan=gan)\n",
        "\n",
        "local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)"
      ],
      "metadata": {
        "id": "eeLA35rgY4T6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin training"
      ],
      "metadata": {
        "id": "p7mImrSvX1k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100  # In practice, use ~100 epochs\n",
        "\n",
        "with strategy.scope():\n",
        "  random_latent_vectors_monitor = tf.constant(gen_var.numpy())\n",
        "\n",
        "  if ckpt_manager.latest_checkpoint:\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir), options=local_device_option)\n",
        "  gan.compile(\n",
        "      d_optimizer=discriminator_optimizer,\n",
        "      g_optimizer=generator_optimizer,\n",
        "      loss_fn=keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM),\n",
        "  )\n",
        "\n",
        "  gan.fit(\n",
        "      dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        "  )"
      ],
      "metadata": {
        "id": "PDZISvNs2_1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa3fc53-8127-4daf-a80d-7354831049ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2970: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6332/6332 [==============================] - 293s 44ms/step - d_loss: 5.0003 - g_loss: 4.6310\n",
            "Epoch 2/100\n",
            "6332/6332 [==============================] - 260s 41ms/step - d_loss: 5.4035 - g_loss: 3.8629\n",
            "Epoch 3/100\n",
            "6332/6332 [==============================] - 262s 41ms/step - d_loss: 5.2692 - g_loss: 3.5629\n",
            "Epoch 4/100\n",
            "6332/6332 [==============================] - 262s 41ms/step - d_loss: 4.5341 - g_loss: 2.0242\n",
            "Epoch 5/100\n",
            "6332/6332 [==============================] - 263s 42ms/step - d_loss: 3.3261 - g_loss: 0.6929\n",
            "Epoch 6/100\n",
            "6332/6332 [==============================] - 262s 41ms/step - d_loss: 2.5361 - g_loss: 0.5041\n",
            "Epoch 7/100\n",
            "6332/6332 [==============================] - 262s 41ms/step - d_loss: 2.2531 - g_loss: 0.3972\n",
            "Epoch 8/100\n",
            "6332/6332 [==============================] - 261s 41ms/step - d_loss: 2.1342 - g_loss: 0.3671\n",
            "Epoch 9/100\n",
            "6332/6332 [==============================] - 261s 41ms/step - d_loss: 1.9313 - g_loss: 0.3009\n",
            "Epoch 10/100\n",
            "6332/6332 [==============================] - 265s 42ms/step - d_loss: 1.9031 - g_loss: 0.3010\n",
            "Epoch 11/100\n",
            "6332/6332 [==============================] - 263s 42ms/step - d_loss: 1.8600 - g_loss: 0.3056\n",
            "Epoch 12/100\n",
            "6332/6332 [==============================] - 263s 41ms/step - d_loss: 1.6996 - g_loss: 0.2986\n",
            "Epoch 13/100\n",
            "6332/6332 [==============================] - 264s 42ms/step - d_loss: 1.6140 - g_loss: 0.3432\n",
            "Epoch 14/100\n",
            "1617/6332 [======>.......................] - ETA: 3:16 - d_loss: 1.3510 - g_loss: 0.2060"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save to GIF"
      ],
      "metadata": {
        "id": "avqLvUimXV27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anim_file = 'dcgan0.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "    filenames = glob.glob('generated_img*_0.png')\n",
        "    filenames = sorted(filenames)\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        image = cv2.resize(image, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
        "        writer.append_data(image)\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)"
      ],
      "metadata": {
        "id": "2TR7MokZ3Btf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}